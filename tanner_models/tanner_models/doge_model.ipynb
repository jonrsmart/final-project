{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "endpoint = 'https://min-api.cryptocompare.com/data/histoday'\n",
    "res = requests.get(endpoint + '?fsym=DOGE&tsym=USD&limit=2000')\n",
    "df = pd.DataFrame(json.loads(res.content)['Data'])\n",
    "df.drop(['conversionType'], 1, inplace=True)\n",
    "df.drop(['conversionSymbol'], 1, inplace=True)\n",
    "df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "df = df.set_index(\"time\")[['close']].tail(1000)\n",
    "df = df.set_index(pd.to_datetime(df.index))\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(results):\n",
    "    \"\"\"\n",
    "    Plots the loss and accuracy for the training and testing data\n",
    "    \"\"\"\n",
    "    history = results.history\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.plot(history['acc'])\n",
    "    plt.legend(['val_accuracy', 'accuracy'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(seq, n_steps_in, n_steps_out):\n",
    "    \"\"\"\n",
    "    Splits the univariate time sequence\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(seq)):\n",
    "        end = i + n_steps_in\n",
    "        out_end = end + n_steps_out\n",
    "        \n",
    "        if out_end > len(seq):\n",
    "            break\n",
    "        \n",
    "        seq_x, seq_y = seq[i:end], seq[end:out_end]\n",
    "        \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_maker(n_layers, n_nodes, activation, drop=None, d_rate=.5):\n",
    "    \"\"\"\n",
    "    Create a specified number of hidden layers for an RNN\n",
    "    Optional: Adds regularization option, dropout layer to prevent potential overfitting if necessary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the specified number of hidden layers with the specified number of nodes\n",
    "    for x in range(1,n_layers+1):\n",
    "        model.add(LSTM(n_nodes, activation=activation, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many periods looking back to train\n",
    "n_per_in  = 30\n",
    "\n",
    "# How many periods ahead to predict\n",
    "n_per_out = 10\n",
    "\n",
    "# Features (in this case it's 1 because there is only one feature: price)\n",
    "n_features = 1\n",
    "\n",
    "# Splitting the data into appropriate sequences\n",
    "X, y = split_sequence(list(df.close), n_per_in, n_per_out)\n",
    "\n",
    "# Reshaping the X variable from 2D to 3D\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "model = Sequential()\n",
    "\n",
    "# Activation\n",
    "activ = \"softsign\"\n",
    "\n",
    "# Input layer\n",
    "model.add(LSTM(30, activation=activ, return_sequences=True, input_shape=(n_per_in, n_features)))\n",
    "\n",
    "# Hidden layers\n",
    "layer_maker(n_layers=1, n_nodes=3, activation=activ)\n",
    "\n",
    "# Final Hidden layer\n",
    "model.add(LSTM(10, activation=activ))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(n_per_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "27/27 [==============================] - 14s 217ms/step - loss: 1.0891e-05 - acc: 0.0866 - val_loss: 0.0461 - val_acc: 0.0722\n",
      "Epoch 2/75\n",
      "27/27 [==============================] - 1s 52ms/step - loss: 8.2152e-06 - acc: 0.0970 - val_loss: 0.0451 - val_acc: 0.0722\n",
      "Epoch 3/75\n",
      "27/27 [==============================] - 3s 100ms/step - loss: 8.9994e-06 - acc: 0.1230 - val_loss: 0.0445 - val_acc: 0.0722\n",
      "Epoch 4/75\n",
      "27/27 [==============================] - 3s 102ms/step - loss: 7.0506e-06 - acc: 0.0790 - val_loss: 0.0438 - val_acc: 0.0619\n",
      "Epoch 5/75\n",
      "27/27 [==============================] - 2s 79ms/step - loss: 7.9407e-06 - acc: 0.0803 - val_loss: 0.0428 - val_acc: 0.0722\n",
      "Epoch 6/75\n",
      "27/27 [==============================] - 2s 65ms/step - loss: 8.1955e-06 - acc: 0.0813 - val_loss: 0.0417 - val_acc: 0.0722\n",
      "Epoch 7/75\n",
      "27/27 [==============================] - 2s 93ms/step - loss: 6.9858e-06 - acc: 0.0925 - val_loss: 0.0412 - val_acc: 0.0722\n",
      "Epoch 8/75\n",
      "27/27 [==============================] - 2s 74ms/step - loss: 7.3655e-06 - acc: 0.0913 - val_loss: 0.0412 - val_acc: 0.0722\n",
      "Epoch 9/75\n",
      "27/27 [==============================] - 2s 68ms/step - loss: 7.0440e-06 - acc: 0.0948 - val_loss: 0.0408 - val_acc: 0.1340\n",
      "Epoch 10/75\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 5.3771e-06 - acc: 0.0833 - val_loss: 0.0390 - val_acc: 0.1443\n",
      "Epoch 11/75\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 6.7505e-06 - acc: 0.0937 - val_loss: 0.0375 - val_acc: 0.1753\n",
      "Epoch 12/75\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 6.3249e-06 - acc: 0.0857 - val_loss: 0.0374 - val_acc: 0.1753\n",
      "Epoch 13/75\n",
      "27/27 [==============================] - 2s 90ms/step - loss: 6.7666e-06 - acc: 0.0847 - val_loss: 0.0374 - val_acc: 0.1649\n",
      "Epoch 14/75\n",
      "27/27 [==============================] - 3s 115ms/step - loss: 5.8766e-06 - acc: 0.0712 - val_loss: 0.0373 - val_acc: 0.1753\n",
      "Epoch 15/75\n",
      "27/27 [==============================] - 3s 109ms/step - loss: 5.5949e-06 - acc: 0.0797 - val_loss: 0.0360 - val_acc: 0.1753\n",
      "Epoch 16/75\n",
      "27/27 [==============================] - 2s 87ms/step - loss: 5.9766e-06 - acc: 0.0807 - val_loss: 0.0355 - val_acc: 0.1753\n",
      "Epoch 17/75\n",
      "27/27 [==============================] - 2s 88ms/step - loss: 3.9013e-06 - acc: 0.0979 - val_loss: 0.0352 - val_acc: 0.1340\n",
      "Epoch 18/75\n",
      "27/27 [==============================] - 2s 61ms/step - loss: 5.6564e-06 - acc: 0.0834 - val_loss: 0.0359 - val_acc: 0.1340\n",
      "Epoch 19/75\n",
      "27/27 [==============================] - 2s 92ms/step - loss: 4.0000e-06 - acc: 0.1103 - val_loss: 0.0349 - val_acc: 0.1340\n",
      "Epoch 20/75\n",
      "27/27 [==============================] - 2s 79ms/step - loss: 4.0417e-06 - acc: 0.0864 - val_loss: 0.0340 - val_acc: 0.1031\n",
      "Epoch 21/75\n",
      "27/27 [==============================] - 2s 73ms/step - loss: 2.9343e-06 - acc: 0.0948 - val_loss: 0.0335 - val_acc: 0.1546\n",
      "Epoch 22/75\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 3.5619e-06 - acc: 0.0877 - val_loss: 0.0335 - val_acc: 0.1443\n",
      "Epoch 23/75\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 3.9372e-06 - acc: 0.0865 - val_loss: 0.0334 - val_acc: 0.1340\n",
      "Epoch 24/75\n",
      "18/27 [===================>..........] - ETA: 0s - loss: 2.6108e-06 - acc: 0.1043"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = 'MeanSquaredError', metrics=[\"acc\"] )\n",
    "res = model.fit(X, y, epochs=75, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Getting predictions by predicting from the last available X variable\n",
    "yhat = model.predict(X[-1].reshape(1, n_per_in, n_features)).tolist()[0]\n",
    "\n",
    "# Transforming values back to their normal prices\n",
    "yhat = scaler.inverse_transform(np.array(yhat).reshape(-1,1)).tolist()\n",
    "\n",
    "# Getting the actual values from the last available y variable which correspond to its respective X variable\n",
    "actual = scaler.inverse_transform(y[-1].reshape(-1,1))\n",
    "\n",
    "# Printing and plotting those predictions\n",
    "print(\"Predicted Prices:\\n\", yhat)\n",
    "plt.plot(yhat, label='Predicted')\n",
    "\n",
    "# Printing and plotting the actual values\n",
    "print(\"\\nActual Prices:\\n\", actual.tolist())\n",
    "plt.plot(actual.tolist(), label='Actual')\n",
    "\n",
    "plt.title(f\"Predicted vs Actual Closing Prices\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.savefig(\"BTC_validation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting off of y because it contains the most recent dates\n",
    "yhat = model.predict(np.array(df.tail(n_per_in)).reshape(1, n_per_in, n_features)).tolist()[0]\n",
    "\n",
    "# Transforming the predicted values back to their original prices\n",
    "yhat = scaler.inverse_transform(np.array(yhat).reshape(-1,1)).tolist()\n",
    "\n",
    "# Creating a DF of the predicted prices\n",
    "preds = pd.DataFrame(yhat, index=pd.date_range(start=df.index[-1], periods=len(yhat), freq=\"D\"), columns=df.columns)\n",
    "\n",
    "# Printing the predicted prices\n",
    "print(preds)\n",
    "\n",
    "# Number of periods back to visualize the actual values\n",
    "pers = 10\n",
    "\n",
    "# Transforming the actual values to their original price\n",
    "actual = pd.DataFrame(scaler.inverse_transform(df[[\"close\"]].tail(pers)), index=df.close.tail(pers).index, columns=df.columns).append(preds.head(1))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(actual, label=\"Actual Prices\")\n",
    "plt.plot(preds, label=\"Predicted Prices\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.title(f\"Forecasting the next {len(yhat)} days\")\n",
    "plt.legend()\n",
    "plt.savefig(\"eth_predictions.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
